
https://coolshell.cn/articles/17459.html  昨儿耗子


就是要让我们的计算环境（包括软硬件）做到full-time的可用性。

## 做好设计
- 对软硬件的冗余，以消除单点故障。任何系统都会有一个或多个冗余系统做standby
- 对故障的检测和恢复。检测故障以及用备份的结点接管故障点。这也就是failover
- 需要很可靠的交汇点（CrossOver）。这是一些不容易冗余的结点，比如域名解析，负载均衡器等。

听起似乎很简单吧，然而不是，细节之处全是魔鬼，冗余结点最大的难题就是对于有状态的结点的数据复制和数据一致性的保证（无状态结点的冗余相对比较简单）。冗余数据所带来的一致性问题是魔鬼中的魔鬼：

如果系统的数据镜像到冗余结点是异步的，那么在failover的时候就会出现数据差异的情况。
如果系统在数据镜像到冗余结点是同步的，那么就会导致冗余结点越多性能越慢。

面，总结一下高可用的设计原理：

要做到数据不丢，就必需要持久化
要做到服务高可用，就必需要有备用（复本），无论是应用结点还是数据结点
要做到复制，就会有数据一致性的问题。
我们不可能做到100%的高可用，也就是说，我们能做到几个9个的SLA。


数据不丢 -> 持久化 -> 服务高可用 -> 备份 -> 一致性


## 标准
1. 99.9% 99.99

## SLA
1. https://en.wikipedia.org/wiki/Service-level_agreement
2. 一个是故障发生到恢复的时间
![](./images/dis01.png)

比如，99.999%的可用性，一年只能有5分半钟的服务不可用。感觉很难做到吧。

就算是3个9的可用性，一个月的宕机时间也只有40多分钟，看看那些设计和编码不认真的团队，把所有的期望寄托在人肉处理故障的运维团队， 一个故障就能处理1个多小时甚至2-3个小时，连个自动化的工具都没有，还好意思在官网上声明自己的SLA是3个9或是5个9，这不是欺骗大众吗？。



## 设计阶段
1. 简化系统设计
        不要过早的进行服务化拆分
2. 尽可能了解系统全局和数据指标 -> 上周的数据
3. 选择合适的组件 性能，稳定性，适配行，社区熟悉度，项目活跃度
4. 规范设计，接口设计形成文档
## 研发阶段
1. 容错设计 + 重试
    重试放大 A->B->C
    c有问题，b重试三次返回a,a又回重试3次 放大 9次
    限制重试 -> b告诉a,我已经重试，你不需要重试了
2. code review
3. 幂等
    a. 乐观锁 -> where total = 100 的时候才给他+50
    b. 去重表 -> 唯一数据写入到表
    c. 全局唯一性检查 -> 生成全局唯一ID
## 部署
变更 = 故障
规范上线时间
1. 上线时间，频率 周五不上线 ，6点之前上线
2. 完整的checklist 和必要的业务开关，非常重要！！！
3. 上线周知和观察
4. 预热和慢启动 -> JIT -> 慢慢放量
灰度发布
1. 按照服务池灰度
2. 按照策略放量
## 容量评估
1. 确定流量来源和目标值
2. 梳理链路资源和服务情况
3. 监控+ 全链路压测
4. 确定水位
5. 确定机器数量和资源数量
## 压力测试
1. 真实数据 -> 线上流量copy -> mock服务 -> 影子存储 -> 不影响线上数据
2. 流量标记
3. 压测报告
## 如何减少故障发生
1. 冗余 
    同步复制 -> mysql -> 所有主同写完
    半同步复制 -> kafka -> isr
    异步复制 -> redis
2. CDN冗余

## 负载均衡
1. 轮询
2. 加权轮询
3. 待地址/urlhash
4. JSQ -> client 标记后端节点的请求数量 -> client 视角看，对于全局可能不是

## 节点容错
1. 故障转移
2. 节点检测

