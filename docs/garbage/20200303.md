

## Kafka

mq ： 解耦 异步 削峰

消息队列 Client A -> queue -> Client B subscription 

pub sub 发布订阅模式

Kafka scala 

Kafka 是一个分布式的消息队列。 topic partition replication

producer consumer broker kafka 集群由多个 kafka 实例组成

kafka 集群 依赖 zk保存一些 meta信息

topic partition consumer producer 

一个 topic 可以被 一个 consumber group consume ，一个 partition 只能被一个 同一个组中一个consumer

进行消费，zk注册信息

topic 可以理解为一个队列

一个topic 可以有多个 consumber group ，topic 的消息复

一个 kafka 机器就是 一个 broker ,一个集群 由多个 broker组成，一个 broker 可以容纳 多个 topic

partition ： 一个topic 可以分布在 多个 broker上，一个 topic可以分为多个 partition ，每个partion是一个
有序的队列，offset 。partition 中每个消息都有一个 在 本 partition 的 offset .

partition 存储文件都是按照 offset.kafka来命名 ，用 offset 就是方便查找。

kafka 的集群是一个 伪集群，通过 修改 配置文件，在启动的时候指定不同的配置文件进行的

broker.id
host
port 

并且配置 log.dirs 配置 kafka的数据路径。log.dir 虽然名字像是 log，但是是数据的存储的地方

broker.id 是全局唯一的不可重复的

日志过期时间，zk的地址什么的

kafka 的 备份不负责读写，只是负责备份

通过命令 ./bin/kafka-server-start.sh ./config/server.properties启动kafka实例

producer 采用 push 模式 将消息发布到 broker ，每条消息被追加到 分区，属于顺序写磁盘，效率比较高，保证了 
kafka的吞吐率。

partition : 消息发送的时候都会发送到 一个 topic ，其本质是一个目录，而topic 是一些 partition  logs组成

partition offset HW,EOL 每个partition 的消息都是有序的，生产的消息被不断的追加到 partition log

分区的原因：可以适应不同的机器，提高并行读的能力。

Producer 分区的原则：

指定了分区，直接使用
没有指定分区，指定了key，根据 key 的hash进行分区
分区和key都没有指定，轮询出要给 分区

同一个 partition 可能由多个 replication 对应 server.properties 配置中 

default.replication.factor=N

leader 一旦挂掉，需要 在 replication 中选出一个 leader 

producer 先从 zk 找到 该 partition 的leader

之后好像都不适用zk，bootstrap-server

producer 将消息发送给该 leader
leader 将日志写入 本地 log
follower 从 leader pull 消息，写入本地的log后向 leader 发送 ack.

ISR 中的 replication 的ack ，向 producer 提交 ack

ISR 通信延迟短，为什么不使用 条数，12 batch,10条的量，


broker 保存消息， 

存储方式
topic 被分成 一个或者多个 partition ,每个partition 是一个文件，文件内部存者索引和 offset开头的log文件

消息被删出 可以配置

删除旧数据，可以基于时间 基于 大小。 Kafka读取特定消息的时间复杂度是 o1 ，这里 删除 过期的文件
并不会提升 kafka的性能。

高级 API ,不需要 自行管理 offset,通过zk管理，不需要管理分区，副本情况

消费者 以 consumer group 消费者组的方式工作，由一个或者多个消费者组成一个组，
共同消费 一个 topic ，多个 消费者组 可以同时消费这个 分区。

消费方式

consumer 采用 pull 模式从 broker 中读取数据。

可以自己控制速度。

## ES

ElasticSearch Logstash kibana

beat file beat Elastic Stack

ELK

Elastic Stack 的组成：

filebeat packetbeat metricbeat packbeat heartbeat

es logstash kibana

ES 基于 Java ，是个开源的分布式搜索引擎，分布式，零配置，自动发现，索引自动分片，分片 备份

restful接口，
logstash 基于 Java ，开源的用于 日志手机，分析 和 存储日志的工具。

ES 基于 Lucene 的搜索服务器。基于 RESTFul 接口，设计用于 云计算中，能够达到 实时搜索，稳定，可靠，快速，

ES 使用Java 开发的。

ES6.5 版本

vim conf/elasticsearch.yml 
network.host:0.0.0.0 # 设置 ip地址，任意的网络均可访问

配置 jvm的参数 修改 初始 堆内存 ，和 最大堆内存

vim /etc/sysctl.conf 

./bin/elasticsearch 启动 es

安装 es-head

ES 基本概念

索引 index 是 es 对逻辑数据的逻辑存储，逻辑存储，所以，它可以分为更小的部分

索引可以看作 关系数据库的表，素银的结构是为快速有效的全文索引准备的。

es可以把索引存储在多台机器上，每个索引都有 一个或者多个分片，每个分片有多个副本。

索引 是 ES 对逻辑数据

文档，，存储在 ES的实体是 文档。用关系型数据库类比，相当于一条记录。

文档可以有不同的结构，但是在es中，相同的字段必须有相同的类型。

文档有多个字段组成，每个字段可能多次出现在一个文档中，这种字段叫做 多值字段。

所有文档 写进索引之前 都会先进行分析，如何将 文本分割为词条，哪些词条 又会被过滤，这种行为叫做映射。

// 创建非结构化的索引

{
    "settings":{
        "index ": {
            "number_os_shards"
        }
    }
}

DELETE /haoke

POST /haoke/user/1001

{
    "id":1001,
    "name":"zhansan",
    "age":20,
    "sex": "男"
}

不指定 id插入数据

POST /haoke/user/
不需要再 url指定id ,那么它会自动生成一个 id。

PUT,指定 id 可以覆盖更新数据。。

会更新 version

局部更新

POST /haoke/user/1001/_update

{
    "doc" {
        age:12
    }
}

删除数据。

在 ES中，删除文档的数据，只要发起 DELETE请求即可

删除的时候只是标记删除，找不到删除的数据会返回 404

搜索数据：

GET /haoke/user/xxxxxxxx

请求数据。

GET /haoke/user/_search

响应十条 数据。默认返回 10条

shards 分片数据

hits 命中数据。

index,type,score,source 

查询年龄 等于 20岁的用户

GET /haoke/user/_search?q=age:20

_search?q=age:20
_searcch?q=age:20

GET /haoke/user/_search?q=age:20

hits 命中 total1 score

DSL query DSL 可以构建 更加复杂，强大的查询。

Domain Specific Language 特定语言，
可以构建复杂的查询操作 

POST /haoke/user/_search
# 请求体
{
    "query":{
        "match":{ # match 只是查询的一种
            "age" : 20
        }
    }
}

POST /haoke/user/_search

{
    "query" :{
        "bool": {
            "filter":{
                "range":age gt 30
            }
            
                    }
    }
}

//模糊查询数据

POST /haoke/user/_search

{
    "query":{
        "match" :{
            "name" :"张三 李四"
        }
    }
    highlight {

    }
}

// 聚合

ES 中支持 聚合操作，相当于 SQL 中的 groupby操作

Aggregation 

Aggregation

Aggregation

聚合

{
    "aggs" :{
        "all_interests":{
            "terms" :{
                "field" : "age"
            }
        }
    }
}

_index 相当于表
_type 文档代表的类
_id 文档的唯一标识

我们的数据 和索引被存储在分片中。，索引只是把一个或者多个分片分组在一起的逻辑空间。
_type 在应用中，我们使用对象表示一些事物，一个用户，一篇博客，一个评论，每个对象都属于 class


_type 名字可以是 大写 或者 小写，不能把傲寒下划线 或者 都好。
id仅仅是一个字符串，是在 es中唯一标识的一个文档。 如果没有指定，会自动帮你生成 32位。

pretty 可以美化响应，格式化 json

pretty

指定响应的字段


/user/_search?q=age:20 这个是根据字段查询

/user/1003?_source=id,name 这是只返回 有用的数据。

在响应的数据中，我们不需要全部的字段，我们指定某些 需要的字段进行返回。

_source 只是返回原始数据

/user/1003/_source?_source=id,name

// 判断文档是否存在

HEAD /haoke/user/1003

不需要返回值 的请求，只需要一个 status code

batch 批量操作

POST /haoke/user/_mget
{
    "ids":["1001","1003"]
}

POST /haoke/user/_mget
{
    "ids":["ss","aa"]
}

如果有一条数据不存在，不会中断，通过返回值中 found 字段判断是否可以。

_bulk 操作 ，需要换行。

在 es中，支持 批量的插入 修改，删除操作。

分页

LIMIT from size  es from size

size 默认 10 from  默认 0

GET /_search?size=5&from=5

当心 分页太深或者一次请求太多的结果，结果返回之前会被排序，但是一个搜索请求常常涉及到多个 分片，

每个分片生成自己排序号的结果，他们需要集中起来排序 确保整体排序正确。

GET /haoke/user/_search?size=1&from=2

在集群系统的深度分页，可能导致 做 很多浪费的工作，效率低下。因为 它要进行内部前 N 的排序，然后 总体排序，抛弃 很多的
数据返回。

String  string text keyword
byte short int long
float double
boolean date

新版本 text keyword

text 当一个全文字段要被 权威搜索，比如 产品描述 等 ，设置为 text 类型，字段容易被分析，
keyword 设用于 索引结构化的字段，比如 email 地址，主机名，keyword 类型的字段只能通过精确值查找。

创建明确类型的索引

GET /haoke/_mapping

GET /haoke/person/_search
query   
    match
        hobby

结构化的精确查询

_search
query
    term 
        age 20
terms 匹配多个条件



