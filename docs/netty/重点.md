NioEventLoopGroup -> EpollEventLoopGroup
NioServerSocketChannel -- > EpollServerSocketChannel

水平触法
边缘触发

1:JDK NIO 传输模型默认采用水平触发，而netty Epoll传输模型默认是边缘触发。一般情况下边缘触发的性能会优于水平触发。
2:采用Epoll传输模型具有更少的GC。
3: Epoll传输模型采用了更多Linux下的本地传输，性能会远高于传输基于java提供的异步/非阻塞网络编程的抽象。


水平触发(LT)：只要文件描述符关联的读内核缓冲区非空，有数据可以读取，就一直发出可读信号进行通知。
边缘触发(FT)：当文件描述符关联的读内核缓冲区由空转化为非空的时候，则发出可读信号进行通知。当文件描述符关联的内核写缓冲区由满转化为不满的时候，则发出可写信号进行通知

## 粘包 拆包
1. tcp socket 缓冲区的链接复用 导致杂乱
2. 节省流量算法Nagle TCP_NODELAY = 1  .childOption(ChannelOption.TCP_NODELAY, true) // 随时发送，不等待
3. 接收端来不及接收
拆
1. 报文太大 1460bytes
2. UDP不拆包不限制，是 ip限制组装
### 如何处理
1. 定长消息
2. 分割符
3. head指定消息长度


## 慢启动
1. 可以关闭
2. 启动除了三次握手 + 初次耗时增加

## 拥塞控制

通告窗口：
技术解释：TCP流量控制的方法之一，在TCP两端在三次握手时就会声明自己接受窗口的大小来提供，窗口大小为字节数，每次发送ack确认数据包同时会传送当前的窗口大小，发送方发送的数据量不可以超过接收端窗口的大小，当窗口为0时，发送方将停止数据的发送


拥塞窗口：发送方为一个动态变化的窗口叫做拥塞窗口，拥塞窗口的大小取决于网络的拥塞程度。发送方让自己的发送窗口=拥塞窗口，但是发送窗口不是一直等于拥塞窗口的，在网络情况好的时候，拥塞窗口不断的增加，发送方的窗口自然也随着增加，但是接受方的接受能力有限，在发送方的窗口达到某个大小时就不在发生变化了。

    发送方如果知道网络拥塞了呢？发送方发送一些报文段时，如果发送方没有在时间间隔内收到接收方的确认报文段，则就可以人为网络出现了拥塞。

    慢启动算法的思路：主机开发发送数据报时，如果立即将大量的数据注入到网络中，可能会出现网络的拥塞。慢启动算法就是在主机刚开始发送数据报的时候先探测一下网络的状况，如果网络状况良好，发送方每发送一次文段都能正确的接受确认报文段。那么就从小到大的增加拥塞窗口的大小，即增加发送窗口的大小。

    例子：开始发送方先设置cwnd（拥塞窗口）=1,发送第一个报文段M1，接收方接收到M1后，发送方接收到接收方的确认后，把cwnd增加到2，接着发送方发送M2、M3，发送方接收到接收方发送的确认后cwnd增加到4，慢启动算法每经过一个传输轮次（认为发送方都成功接收接收方的确认），拥塞窗口cwnd就加倍。


## zero copy

总结：
1. 用户态 -> 内核地址映射，减少上下文切换和copy mmap mappedbytebuffer
2. sendfile socket存储内核 地址便宜 ，避免copy
3. netty Composite

netty,nio,kafka,rocketmq 
### 普通

      用户态    内核    磁盘
read -> check 内核换冲区 
                -> 有 -> copy 进程缓冲区
                -> 没有 -> DMA -> 内核缓冲区 -> 进程缓冲区

### mmap
把内核空间地址和用户空间的虚拟地址映射到同一个物理地址
省去了内核与用户空间的往来拷贝

### sendfile

将Kernel buffer中对应的数据描述信息（内存地址，偏移量）记录到相应的socket缓冲区当中，这样连内核空间中的一次cpu copy也省掉了

### java 和netty 中的zerocopy

java
1. MappedByteBuffer
2. DirectByteBuffer
3. Channel-to-Channel
netty

Composite(组合)和Slice(拆分)

https://mp.weixin.qq.com/s/XT4l0PBS9ilUXEE313rBVA
https://juejin.cn/post/6844903815913668615